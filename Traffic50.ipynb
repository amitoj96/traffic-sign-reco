{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_val = 'data/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import keras.preprocessing.image.ImageDataGenerator\n",
    "train_datagen =  image.ImageDataGenerator(rotation_range=5, width_shift_range=0.08, shear_range=0.1,\n",
    "                               height_shift_range=0.1, zoom_range=0.1)\n",
    "test_datagen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35288 images belonging to 43 classes.\n",
      "Found 3921 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(50, 50),\n",
    "        batch_size=48,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        path_val,\n",
    "        target_size=(50, 50),\n",
    "        batch_size=48,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_bn():\n",
    "    model = Sequential([\n",
    "        Lambda(get_x,input_shape=(3,50,50)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        #Dense(4096, activation='relu'),\n",
    "        #Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        Dense(43, activation='softmax')\n",
    "        ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 50, 50)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_2_model\" was not an Input tensor, it was generated by layer lambda_2.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_2\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.8320 - acc: 0.7635 - val_loss: 0.0622 - val_acc: 0.9801\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.1406 - acc: 0.9565 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.0880 - acc: 0.9737 - val_loss: 0.0404 - val_acc: 0.9860\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 433s - loss: 0.0789 - acc: 0.9757 - val_loss: 0.0095 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0558 - acc: 0.9827 - val_loss: 0.0067 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.0565 - acc: 0.9832 - val_loss: 0.0390 - val_acc: 0.9865\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0592 - acc: 0.9827 - val_loss: 0.0117 - val_acc: 0.9964\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 433s - loss: 0.0591 - acc: 0.9831 - val_loss: 0.0118 - val_acc: 0.9962\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.0486 - acc: 0.9857 - val_loss: 0.0062 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.0331 - acc: 0.9897 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 434s - loss: 0.0552 - acc: 0.9843 - val_loss: 0.0181 - val_acc: 0.9944\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0539 - acc: 0.9849 - val_loss: 0.0096 - val_acc: 0.9982\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 433s - loss: 0.0367 - acc: 0.9890 - val_loss: 0.0042 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.0241 - acc: 0.9929 - val_loss: 0.0051 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 433s - loss: 0.0295 - acc: 0.9919 - val_loss: 0.0123 - val_acc: 0.9964\n"
     ]
    }
   ],
   "source": [
    "modelA = get_model_bn()\n",
    "for i in range(10,25):\n",
    "    modelA.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=train_generator.n,\n",
    "        nb_epoch=1,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=validation_generator.n)\n",
    "    modelA.save_weights(\"data/tr50A\" + str(i) + \".h5\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_3 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 50, 50)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_3_model\" was not an Input tensor, it was generated by layer lambda_3.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_3\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 426s - loss: 0.7838 - acc: 0.7770 - val_loss: 0.0496 - val_acc: 0.9852\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.1344 - acc: 0.9586 - val_loss: 0.0365 - val_acc: 0.9893\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 426s - loss: 0.0960 - acc: 0.9702 - val_loss: 0.0230 - val_acc: 0.9934\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0758 - acc: 0.9769 - val_loss: 0.0147 - val_acc: 0.9946\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0606 - acc: 0.9814 - val_loss: 0.0225 - val_acc: 0.9926\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0544 - acc: 0.9834 - val_loss: 0.0343 - val_acc: 0.9875\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0523 - acc: 0.9835 - val_loss: 0.0108 - val_acc: 0.9964\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0465 - acc: 0.9855 - val_loss: 0.0065 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0415 - acc: 0.9874 - val_loss: 0.0066 - val_acc: 0.9980\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0361 - acc: 0.9889 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 426s - loss: 0.0353 - acc: 0.9899 - val_loss: 0.0107 - val_acc: 0.9982\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0338 - acc: 0.9902 - val_loss: 0.0035 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 426s - loss: 0.0436 - acc: 0.9880 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0327 - acc: 0.9903 - val_loss: 0.0543 - val_acc: 0.9832\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 425s - loss: 0.0279 - acc: 0.9913 - val_loss: 0.0030 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0298 - acc: 0.9918 - val_loss: 0.0071 - val_acc: 0.9977\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0284 - acc: 0.9918 - val_loss: 0.0032 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 426s - loss: 0.0197 - acc: 0.9944 - val_loss: 0.0202 - val_acc: 0.9936\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 425s - loss: 0.0257 - acc: 0.9929 - val_loss: 0.0067 - val_acc: 0.9977\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0232 - acc: 0.9931 - val_loss: 0.0062 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0217 - acc: 0.9936 - val_loss: 0.0073 - val_acc: 0.9982\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0271 - acc: 0.9921 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0227 - acc: 0.9937 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0220 - acc: 0.9937 - val_loss: 0.0069 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0166 - acc: 0.9951 - val_loss: 0.0066 - val_acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "modelB = get_model_bn()\n",
    "for i in range(25):\n",
    "    modelB.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=train_generator.n,\n",
    "        nb_epoch=1,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=validation_generator.n)\n",
    "    modelB.save_weights(\"data/tr50B\" + str(i) + \".h5\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_4 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 50, 50)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_4_model\" was not an Input tensor, it was generated by layer lambda_4.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_4\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.8348 - acc: 0.7658 - val_loss: 0.0436 - val_acc: 0.9862\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 433s - loss: 0.1488 - acc: 0.9542 - val_loss: 0.0232 - val_acc: 0.9929\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0835 - acc: 0.9743 - val_loss: 0.0087 - val_acc: 0.9977\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0733 - acc: 0.9775 - val_loss: 0.0683 - val_acc: 0.9773\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0665 - acc: 0.9795 - val_loss: 0.0111 - val_acc: 0.9957\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0494 - acc: 0.9846 - val_loss: 0.0323 - val_acc: 0.9878\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0646 - acc: 0.9809 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0446 - acc: 0.9863 - val_loss: 0.0148 - val_acc: 0.9959\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0425 - acc: 0.9868 - val_loss: 0.0037 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0386 - acc: 0.9879 - val_loss: 0.0029 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0352 - acc: 0.9899 - val_loss: 0.0020 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0458 - acc: 0.9870 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0269 - acc: 0.9920 - val_loss: 0.0131 - val_acc: 0.9952\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0325 - acc: 0.9904 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0304 - acc: 0.9909 - val_loss: 0.0022 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0228 - acc: 0.9930 - val_loss: 0.0037 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0280 - acc: 0.9921 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0244 - acc: 0.9927 - val_loss: 0.0206 - val_acc: 0.9954\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0285 - acc: 0.9917 - val_loss: 0.0029 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 428s - loss: 0.0249 - acc: 0.9935 - val_loss: 0.0023 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0207 - acc: 0.9938 - val_loss: 0.0082 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0281 - acc: 0.9924 - val_loss: 0.0033 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0244 - acc: 0.9932 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0203 - acc: 0.9942 - val_loss: 0.0017 - val_acc: 0.9995\n"
     ]
    }
   ],
   "source": [
    "modelC = get_model_bn()\n",
    "for i in range(25):\n",
    "    modelC.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=train_generator.n,\n",
    "        nb_epoch=1,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=validation_generator.n)\n",
    "    modelC.save_weights(\"data/tr50C\" + str(i) + \".h5\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_5 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 50, 50)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_5_model\" was not an Input tensor, it was generated by layer lambda_5.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_5\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.8894 - acc: 0.7495 - val_loss: 0.1008 - val_acc: 0.9707\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 432s - loss: 0.1526 - acc: 0.9537 - val_loss: 0.0238 - val_acc: 0.9926\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0956 - acc: 0.9711 - val_loss: 0.0266 - val_acc: 0.9911\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0761 - acc: 0.9783 - val_loss: 0.0604 - val_acc: 0.9809\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0674 - acc: 0.9799 - val_loss: 0.0320 - val_acc: 0.9908\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0603 - acc: 0.9813 - val_loss: 0.0300 - val_acc: 0.9913\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 427s - loss: 0.0538 - acc: 0.9836 - val_loss: 0.0098 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0724 - acc: 0.9790 - val_loss: 0.0355 - val_acc: 0.9903\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0376 - acc: 0.9891 - val_loss: 0.0028 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0376 - acc: 0.9886 - val_loss: 0.0141 - val_acc: 0.9949\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0400 - acc: 0.9879 - val_loss: 0.1194 - val_acc: 0.9653\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0531 - acc: 0.9842 - val_loss: 0.0083 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 429s - loss: 0.0300 - acc: 0.9906 - val_loss: 0.0111 - val_acc: 0.9969\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0296 - acc: 0.9913 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0154 - val_acc: 0.9962\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0304 - acc: 0.9914 - val_loss: 0.0091 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 430s - loss: 0.0294 - acc: 0.9909 - val_loss: 0.0041 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 440s - loss: 0.0273 - acc: 0.9926 - val_loss: 0.0086 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0229 - acc: 0.9929 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0228 - acc: 0.9930 - val_loss: 0.0029 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 431s - loss: 0.0268 - acc: 0.9922 - val_loss: 0.0065 - val_acc: 0.9974\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 434s - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0028 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0291 - acc: 0.9918 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 442s - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 441s - loss: 0.0210 - acc: 0.9942 - val_loss: 0.0010 - val_acc: 0.9995\n"
     ]
    }
   ],
   "source": [
    "modelD = get_model_bn()\n",
    "for i in range(25):\n",
    "    modelD.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=train_generator.n,\n",
    "        nb_epoch=1,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=validation_generator.n)\n",
    "    modelD.save_weights(\"data/tr50D\" + str(i) + \".h5\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_6 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 50, 50)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_6_model\" was not an Input tensor, it was generated by layer lambda_6.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_6\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.8410 - acc: 0.7648 - val_loss: 0.0687 - val_acc: 0.9816\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 448s - loss: 0.1464 - acc: 0.9561 - val_loss: 0.1269 - val_acc: 0.9546\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 445s - loss: 0.0948 - acc: 0.9714 - val_loss: 0.0183 - val_acc: 0.9954\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 446s - loss: 0.0852 - acc: 0.9736 - val_loss: 0.0239 - val_acc: 0.9931\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 445s - loss: 0.0737 - acc: 0.9778 - val_loss: 0.0255 - val_acc: 0.9906\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0553 - acc: 0.9835 - val_loss: 0.0144 - val_acc: 0.9952\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.0462 - acc: 0.9859 - val_loss: 0.0124 - val_acc: 0.9964\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0488 - acc: 0.9854 - val_loss: 0.0173 - val_acc: 0.9946\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0452 - acc: 0.9867 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 446s - loss: 0.0378 - acc: 0.9883 - val_loss: 0.0047 - val_acc: 0.9982\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.0388 - acc: 0.9889 - val_loss: 0.0161 - val_acc: 0.9954\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0382 - acc: 0.9888 - val_loss: 0.0102 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 444s - loss: 0.0305 - acc: 0.9912 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 442s - loss: 0.0304 - acc: 0.9910 - val_loss: 0.0035 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.0321 - acc: 0.9897 - val_loss: 0.0038 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 448s - loss: 0.0294 - acc: 0.9920 - val_loss: 0.0212 - val_acc: 0.9959\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 442s - loss: 0.0289 - acc: 0.9914 - val_loss: 0.0118 - val_acc: 0.9967\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 441s - loss: 0.0211 - acc: 0.9937 - val_loss: 0.0064 - val_acc: 0.9980\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 457s - loss: 0.0238 - acc: 0.9934 - val_loss: 0.0040 - val_acc: 0.9987\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 452s - loss: 0.0215 - acc: 0.9938 - val_loss: 0.0041 - val_acc: 0.9990\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 439s - loss: 0.0252 - acc: 0.9927 - val_loss: 0.0036 - val_acc: 0.9995\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 436s - loss: 0.0147 - acc: 0.9959 - val_loss: 3.5383e-04 - val_acc: 1.0000\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 449s - loss: 0.0235 - acc: 0.9935 - val_loss: 0.0068 - val_acc: 0.9985\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 443s - loss: 0.0206 - acc: 0.9942 - val_loss: 0.0020 - val_acc: 0.9992\n",
      "Epoch 1/1\n",
      "35288/35288 [==============================] - 438s - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0087 - val_acc: 0.9990\n"
     ]
    }
   ],
   "source": [
    "modelE = get_model_bn()\n",
    "for i in range(25):\n",
    "    modelE.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=train_generator.n,\n",
    "        nb_epoch=1,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=validation_generator.n)\n",
    "    modelE.save_weights(\"data/tr50E\" + str(i) + \".h5\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
